{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "load_dotenv(\"/home/agnese.lombardi/progetto/Pragmatics/api.env\")\n",
    "api_key = os.environ.get('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Define Parameters\n",
    "scenario_list = [\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "phenomenas = [\"IR\", \"IS\", \"ID\", \"IR_Os\", \"IT\", \"VI_IH\", \"VI_IQ\", \"VI_IS\"]\n",
    "path_output = Path(\"/home/agnese.lombardi/progetto/ToM_concordia/analysis\")\n",
    "\n",
    "# System Prompt\n",
    "system_prompt = \"\"\"We request your evaluation of the AI model’s response in relation to the given scenario. Specifically, consider the scenario involving two agents and their beliefs, assessing whether the model-generated effects align coherently with the agents’ actions and context.\n",
    "\n",
    "Evaluate the response based on the following criteria:\n",
    "\n",
    "Social Understanding: Does the model grasp the social dynamics and pragmatic nuances of the scenario?\n",
    "Appropriateness: Is the response contextually relevant and suitable for the scenario?\n",
    "Insightfulness: Does the answer demonstrate a deep understanding of intentions, implicature, deceit, irony, sarcasm, humor, metaphor, etc.?\n",
    "Completeness: How well does the response capture the essential elements of the scenario?\n",
    "Agentivity: Is the model's response coherent with the agents’ attempted actions?\n",
    "Scoring:\n",
    "\n",
    "Assign a score from 1 to 5 for each category.\n",
    "Compute a final rating based on these scores.\n",
    "If no effect is provided, assign 0.\n",
    "Output only a single numeric value representing the final rating (1–5).\"\"\"\n",
    "\n",
    "def build_prompt(df, agent):\n",
    "    scenario = df[\"SharedMem\"] + \" \" + df[\"ScenarioPremise\"]\n",
    "    \n",
    "    if agent == df[\"LcName\"]:  \n",
    "        action = df[\"Model_lc_action\"]\n",
    "        belief = df[\"LcSpecificMem\"]\n",
    "        kevent = df[\"known_effect_lc\"]\n",
    "        uevent = df[\"unknown_effect_ilc\"]\n",
    "    else:\n",
    "        action = df[\"Model_ilc_action\"]\n",
    "        belief = df[\"IlcSpecificMem\"]\n",
    "        kevent = df[\"known_effect_ilc\"]\n",
    "        uevent = df[\"unknown_effect_ilc\"]\n",
    "\n",
    "    return f\"\"\"The scenario of the story is: {scenario}.\n",
    "The {agent}'s belief is: {belief}.\n",
    "Given that the agent performs the following attempted action: {action}.\n",
    "The model-generated effects that you have to rate are: {kevent} and {uevent}.\n",
    "The rating is:\"\"\"\n",
    "\n",
    "# Function to get rating from API\n",
    "def get_completion(prompt, model):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        max_tokens=1,\n",
    "        temperature=0,\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt}, {'role': 'user', 'content': prompt}],\n",
    "        n=1\n",
    "    )\n",
    "    return completion.choices[0].message.content  # Return rating\n",
    "\n",
    "for scenario in scenario_list:\n",
    "    for phen in phenomenas:\n",
    "        output_path = path_output / f'{phen}_Task{scenario}.csv'\n",
    "        output_dir = \"/home/agnese.lombardi/progetto/ToM_concordia/analysis\"\n",
    "        \n",
    "        if not output_path.exists():\n",
    "            continue  \n",
    "\n",
    "        df = pd.read_csv(output_path)           \n",
    "        df[\"Rating_lc\"] = \"\"\n",
    "        df[\"Rating_ilc\"] = \"\"\n",
    "        df[\"Prompt_lc\"] = \"\" \n",
    "        df[\"Prompt_ilc\"] = \"\"        \n",
    "\n",
    "        for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "            agent1, agent2 = row.LcName, row.IlcName\n",
    "            \n",
    "            # Process agent 1 (Speaker)\n",
    "            prompt_lc = build_prompt(row, agent1)\n",
    "            rating_lc = get_completion(prompt_lc, 'gpt-4o-mini')\n",
    "            df.at[index, \"Rating_lc\"] = rating_lc\n",
    "            df.at[index, \"Prompt_lc\"] = prompt_lc\n",
    "\n",
    "            # Process agent 2 (Listener)\n",
    "            prompt_ilc = build_prompt(row, agent2)\n",
    "            rating_ilc = get_completion(prompt_ilc, 'gpt-4o-mini')\n",
    "            df.at[index, \"Rating_ilc\"] = rating_ilc\n",
    "            df.at[index, \"Prompt_ilc\"] = prompt_ilc\n",
    "\n",
    "        output_file = os.path.join(output_dir, f\"{phen}_Task{scenario}_ratings.csv\")\n",
    "        df.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for scenario in scenario_list:\n",
    "    for phen in phenomenas:\n",
    "        output_path = path_output / f'{phen}_Task{scenario}.csv'\n",
    "        df = pd.read_csv(output_path)           \n",
    "        df_prompts = df[['Prompt_lc', 'Prompt_ilc']].copy()\n",
    "        data.append(df_prompts)\n",
    "\n",
    "final_df = pd.concat(data, ignore_index=True)\n",
    "\n",
    "excel_path = \"/home/agnese.lombardi/progetto/ToM_concordia/analysis/prompts.xlsx\"  \n",
    "final_df.to_excel(excel_path, index=False)\n",
    "\n",
    "print(f\"Excel file saved at: {excel_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "progetto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
